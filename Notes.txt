EINLEITUNG
* Bachelorarbeit
* Betreut von
* Blabla
* Refotografie - wasn das?
   + Bezeichnet den Prozess der Rekonstruktion eines Aufnahmestandortes eines
   Bildes
   + Ziel ist es, die Pose und Kameraparameter möglichst genau zu
   rekonstruieren, um erneut ein Bild der selben Szene zu machen
   + Man kann damit gut historischen Wandel dokumentieren, aber auch potenziell
   Baufortschritte oder Jahreszeiten
   + Siehe Beispiele (erregen gelegentlich Medieninteresse aufgrund der starken
   Wirkung)
* Was genau müsste neben der Pose alles rekonstruiert werden?
   // + Abbildung von Welt- in Pixelkoordinaten sieht so aus [GRAFIK ERLÄUTERN]
   // + Die Kameramatrix enthält die Brennweite, den Hauptpunkt und einen
   // Scherungsparameter (üblicherweise vernachläsigt)
   // + Wenn wir annehmen, dass das Weltkoordinatensystem das des Referenzbildes
   // ist, sind wir auf der Suche nach R & T, dann könnten wir uns da hin bewegen
   + Refotografie bisher: Manuell, Timera, rePhoto, Photoshop
   + Frage: Kann man sich nicht durch Computer dabei helfen lassen?
   + Geht, Menschen vom MIT & Adobe haben eine Anwendung für Laptop +
   Digitalkamera entwickelt, die den Prozess laut eigener Aussage ziemlich gut
   erledigt (ist aber unhandlich, wegen Laptop, außerdem nicht verfügbar)
   // + Deren Methode beinhaltet auch die Schätzung der Kalibriermatrix der
   // historischen Kamera
   // + Der User muss das historische Bild manuell registrieren in der aktuellen
   // Szene
   // + Rotation wird vor Präsentation des aktuellen Kamerabildes herausgerechnet

ZIEL DER ARBEIT
* Ziel dieser Arbeit: Ähnliche Funktionalität in prototypischer Weise auf das
   Smartphone bringen
   + Das heißt: Zunächst nur für Referenzbilder, die sich nicht sehr stark
   unterscheiden
   + Wir versuchen es mit automatischer Registrierung der Referenzaufnahme, also
   kein manuelles labeln von Korrespondenzen
   + Es gibt auch keine Schätzung der originalen Kalibriermatrix, wir nehmen
   vereinfachend an, dass für das Referenzbild die selbe Kamera benutzt wurde
   + Zunächst wird versucht, die richtige Translation zu erreichen
   + Alles weitere kann dan hoffentlich durch Weiterentwicklung in der
   Arbeitsgruppe nachgerüstet werden.

VEREINFACHTER ABLAUF
* Wie kann man also vorgehen? Es handelt sich hier um ein
  Structure-from-motion-Szenario, also mit 2 Aufnahmen der selben Szene und
  bewegung dazwischen,
  wobei das Referenzbild die eine 'Kamera' ist und das aktuell von der Kamera
  gelieferte Bild die zweite.
* Die erste Idee, auf die man kommen könnte, sieht ca. so aus [GRAFIK ERLÄUTERN]
   + Man findet irgednwie Korrespondenzen in Referenz und aktuellem Kamerabild
   und findet so heraus, wie die Posedifferenz ist (wie das geht, sag ich gleich)
   + Dann bewegt man sich immer weiter entlang dieses Gradienten, bis man da
   ist.

VERSATZSCHÄTZUNG

* wie bekommt man aus einer Stereoaufnahme (Referenzbild + aktuelles Bild) die
  Posedifferenz?
* [GRAFIK ERLÄUTERN]
* Es gibt eine Bezihung zwischen Bildkoordinaten in Frame 1 und Frame 2:
   + Kameraframes sind durch (R,T) gegeneinander transformiert
   + Bilder der Bildpunkte sind Strahlen, ergo nur bis auf Skalierung bestimmt
   + T-dach ist Matrixschreibweise für das Kreuzprodukt mit T
   + So gelangt man zur Essentiellen Matrix, die einen Punkt auf seine
   Epipolarlinie im anderen Frame abbildet
* Mit Hilfe der Epipolargleichung kann man nun mit Punktkorrespondenzen
  Gleichungen aufstellen, und das resultierenden LGS lösen/optimieren
  + 8-Punkt-Algorithmus: E hat max. 8 DOF wegen Skalierung, also braucht man
  maximal 8 Korrespondenzen
  + in der Realität verrauscht, also braucht man mehr
  + Überbestimmung führt zu einer Lösung mit kleinstem Fehlerquadrat
  + Zudem ist die Lösung nicht unbedingt Essentiell (2 gleiche Singulärwerte) ->
  Projektion auf Raum aller Essentiellen Matrizen
  + Es geht auch mit 5 Punkten, da E eigentlich nur 5 DOF (3 für R, 3 für T, -1
  für Skalierung)
  + Es gibt 4 Lösungen für (R,T), jedoch macht nur eine Sinn, welche die Punkte
  vor beiden Kameras platziert, wenn man ihre 3D-Koordinaten berechnet (nachdem
  man E hat, hat man R und T, sowie K und kann sie dann berechnen).

KORRESPONDENZFINDUNG

* Die Leute vom MIT lassen den User manuell labeln (möglicherweise unersetzlich
  bei historischen, sehr alten Aufnahmen, die nicht viel mit der Realität gemein
  haben)
* In unserem Ansatz versuchen wir es mit automatischer Feature-Erkennung, was
  weniger drastische Veränderungen vorraussetzt und die Anwendbarkeit
  einschränkt. Idealerweise wird die Korrespondenzfindung später so erweitert,
  dass die App mit historischen Bildern klarkommt. (Darüber dürfen andere Leute sich
  dann den Kopf zerbrechen)
* Detektoren gibt es viele, z.B. SURF oder SIFT, die viele Invarianzen haben und
  als verlässlich gelten. Vor allem SIFT ist relativ langsam, SURF ist deutlich
  schneller. Beide benutzen reellwertige Deskriptoren, was beim Matching zu einem
  relativ aufwendigen L2-Norm-Vergleich führt.
* Es wurden daher einige Deskriptoren entwickelt, die Binärstrings sind (BRIEF,
  ORB, KAZE-Deskriptor), die man schneller vergleichen kann, was auf schwächerer
  Hardware von Vorteil ist
* AKAZE ist ein Detektor, der binäre Deskriptoren berechnet, und schneller als
  SURF sein soll bei mindestens so guter Genauigkeit
* Hier muss man auch etwas herumspielen, bis man die richtigen Parameter
  gefunden hat und dabei Performanz gegen Featurezahl und -salienz abwägen
* Es ist aufgrund sehr unterschiedlicher Parametertypen schwer, die Detektoren
  zu verlgeichen, AKAZE scheint zumindest etwas schneller zu sein
* Wir benutzen also AKAZE, um auf den Bildern Korrespondenzen zu schätzen,
  eventuell kann man noch einen Ratio-test machen (tausche Laufzeit gegen Güte)
* Bisher hängen die Ergebnisse noch stark von der Art des Detektors und den
  Parametern ab

Nun haben wir eigentlich alles: Korrespondenzen und Methode zur
Versatzberechnung

PROBLEME DER VERSATZSCHÄTZUNG

* Es so zu machen, wie zuvor geschildert, bringt ein großes Problem mit sich,
  und zwar wird die Versatzschätzung immer ungenauer, je näher die beiden
  Kameras beieinander sind.
* Je näher man dem Ziel kommt, desto weniger weiß man weiter, was natürlich das
  Gegenteil von dem ist, was man will
* Das 2. Problem hat nichts mit dem Naiven Ansatz zu tun, sondern ist ein
  Grundsätzliches, das durch die mathematischen Eigenschaften der projektiven
  Geometrue bedingt ist
  + Man kennt die Skalierung des resultierenden T-Vekors nicht.
  + das wäre normalerweise nicht so schlimm, solange man sich sicher sein
  könnte, dass die Vektoren über die Iterationen hinweg verlgeichbar sind, aber
  das ist so nicht.
  + Wenn ich mit dem aktuellen Frame herauskriege, das Ziel ist links von mir,
  und mit dem nächsten, dann wieder links, dann weiß ich nicht, ob ich näher
  gekommen bin oder mich wieder entfernt habe

LÖSUNG FÜR PROBLEM 1

* Man lässt den Nutzer 2 Bilder der Szene aufnehmen, wobei das eine (aka first
  frame) einigermaßen weit entfernt vom Original ist, die genaue Entfernung
  spielt keine Rolle und das andere die beste Annäherung nach Augenmaß ist
* Man matched das Referenzbild mit dem first frame und speichert sich die
  Translation (mit bestimmter Skalierung, folgt gleich)
* Anstatt das aktuelle mit dem Referenzbild zu vergleichen, um eine
  Nulldifferenz zu erreichen, verlgeichen wir mit dem first frame und versuchen,
  dessen Differenz zum Referenzbild zu erreichen
* Da die 0 nicht das Ziel ist, haben wir das Problem mit der Instabilität nicht.

LÖSUNG FÜR PROBLEM 2

* Die selben beiden Bilder wie zuvor werden nun benutzt, um aus den
  Korrespondenzen eine Punktwolke (3D-Koordinaten) zu rekonstruieren
* Dann berechnet man die mittlere Distanz zwischen den Punkten und dem first
  frame und benutzt dies als Skalierung für T
* 
