EINLEITUNG

* Bachelorarbeit

* Betreut von

* Blabla



REFOTOGRAFIE

* Prozess der Rekonstruktion eines Aufnahmestandortes eines Fotos

* Pose & Kameraparameter möglichst genau rekonstruieren -> Bild der selben Szene machen

* historischen Wandel dokumentieren, Baufortschritte, Jahreszeiten

* Siehe Beispiele (gelegentlich Medieninteresse, starke Wirkung)



* Wie findet man die Pose wieder?

   + bisher: Manuell, Timera, rePhoto, Photoshop

   + Kann man sich nicht durch Computer dabei helfen lassen?

   + Menschen vom MIT & Adobe -> für Laptop + Digitalkamera , laut eigener Aussage ziemlich gut

   + Rekonstruiert alles

   + unhandlich, wegen Laptop, außerdem nicht verfügbar



   // + Deren Methode beinhaltet auch die Schätzung der Kalibriermatrix der

   // historischen Kamera

   // + Der User muss das historische Bild manuell registrieren in der aktuellen

   // Szene

   // + Rotation wird vor Präsentation des aktuellen Kamerabildes herausgerechnet



ZIEL DER ARBEIT

* Ziel: Ähnliche Funktionalität in prototypisch auf Smartphone/Tablet

   + Zunächst nur für Referenzbilder, die sich nicht sehr stark 
   unterscheiden

   + automatische Registrierung der Referenzaufnahme, kein manuelles labeln 

   + keine Schätzung der originalen Kalibriermatrix (Bild könnte ja alt sein) 
      -> wir nehmen an, dass für das Referenzbild die selbe Kamera benutzt wurde

   + Zunächst richtige Translation erreichen

   + Anzeigen und optimieren, eventuell hochladen



   + Alles weitere kann dann hoffentlich durch Weiterentwicklung in der 
   Arbeitsgruppe nachgerüstet werden.



VEREINFACHTER ABLAUF

* Structure-from-motion-Szenario: 2 Aufnahmen der selben Szene, Bewegung dazwischen,

* Referenzbild eine 'Kamera', aktuelles Bild die zweite.

* Erste Idee: sieht ca. so aus [GRAFIK ERLÄUTERN]

   + irgendwie Korrespondenzen in Referenz und aktuellem Kamerabild finden

   + herausfinden, wie die Posedifferenz ist (wie das geht, sag ich gleich)

   + entlang dieses Gradienten bewegen, bis man da ist.



VERSATZSCHÄTZUNG

* wie aus zwei Bildern (Referenzbild + aktuelles Bild) Posedifferenz berechnen?

* [GRAFIK ERLÄUTERN]

   + Umrechnung d. Koordinaten mit R,T 

   + Bildkoordinaten in Beziehung setzen

      + Urbilder der Bildpunkte sind Strahlen -> Skalierung

      + T-dach Matrixschreibweise für Kreuzprodukt mit T

      + Essentielle Matrix, bildet Punkt auf Epipolarlinie im anderen Frame ab

* Mit Epipolargleichung und Punktkorrespondenzen 
  Gleichungen aufstellen, resultierendes LGS lösen/optimieren

  + 8-Punkt-Algorithmus: E hat max. 8 DOF wegen Skalierung -> max 8 Korrespondenzen nötig

  + in der Realität verrauscht, also braucht man mehr

  + Überbestimmung führt zu einer Lösung mit kleinstem Fehlerquadrat

  + Zudem ist die Lösung nicht unbedingt Essentiell (2 gleiche Singulärwerte) -> 
  Projektion auf Raum aller Essentiellen Matrizen

  + Geht auch mit 5 Punkten, da E eigentlich nur 5 DOF (3 für R, 3 für T, -1 
  für Skalierung)

  + 4 Lösungen für (R,T), nur eine geometrisch sinnvoll, platziert 
  Punkte vor beiden Kameras nach 3D-Rekonstruktion

  + T ist leider Einheitsvektor, da Skalierung unbekannt





PROBLEME DER VERSATZSCHÄTZUNG

* So wie gerade sehr problematisch -> Versatzschätzung immer ungenauer, je näher 
  Kameras beieinander.

* Je näher man dem Ziel kommt, desto weniger weiß man weiter -> ziemlich doof

* 2. Problem ist grundsätzlich, wegen der mathematischen Eigenschaften der projektiven 
  Geometrie

  + Man kennt die Skalierung des resultierenden T-Vektors nicht.

  + normalerweise nicht so schlimm, solange Vektoren über die Iterationen hinweg vergleichbar -> is nich

  + Wenn ich mit dem aktuellen Frame herauskriege, das Ziel ist links von mir, 
  und mit dem nächsten, dann wieder links, dann weiß ich nicht, ob näher 
  gekommen oder entfernt 



LÖSUNG FÜR PROBLEM 1



* Man lässt den Nutzer 2 Bilder der Szene aufnehmen, (aka first 
  frame, einigermaßen weit entfernt vom Original; die Entfernung egal)
  und das andere die beste Annäherung nach Augenmaß ist

* Referenzbild mit dem first frame matchen ->  Translation speichern (mit bestimmter
  Skalierung, folgt gleich)

* Anstatt das aktuelles mit Referenzbild vergleichen, um Nulldifferenz zu
  erreichen, lieber aktuelles vergleichen mit first frame und versuchen, 
  dessen Differenz zum Referenzbild zu erreichen

* Da die 0 nicht das Ziel ist, haben wir das Problem mit der Instabilität nicht.



LÖSUNG FÜR PROBLEM 2



* Die selben beiden Bilder benutzen, um aus Korrespondenzen Punktwolke
  (3D-Koordinaten) zu rekonstruieren

* mittlere Distanz zwischen den Punkten und first frame berechnen -> benutzen als
  Skalierung für T

* Mit jeweils aktuellem Frame und first frame das selbe tun,
  Einheits-T skalieren -> Vergleichbarkeit der Vektoren über Iterationen hinweg, alle 
  Rekonstruktionen über first frame verlinkt

* Sinn macht es deshalb, weil die 3D-Koordinaten davon abhängen, wo Punkte aus 
  dem first frame im aktuellen Bild wieder auftauchen. Wir machen die 
  Triangulation ja mit einem Einheits-T; wenn sich nun aber die Korrespondenzen 
  zwischen first frame und aktuellem Bild sehr viel bewegt haben, ist dies nur 
  mit einer größeren Skalierung realisierbar, was sich in den triangulierten 
  Koordinaten widerspiegelt.



KORRESPONDENZFINDUNG



* Leute vom MIT lassen User manuell labeln (evtl. unersetzlich 
  bei historischen alten Aufnahmen)

* wir versuchen es mit automatischer Feature-Erkennung, was 
  weniger Veränderungen voraussetzt und Anwendbarkeit einschränkt. 

  + Idealerweise Korrespondenzfindung später so erweitern, dass App mit
  historischen Bildern klarkommt. 

* Detektoren gibt es viele, z.B. SURF oder SIFT

   + viele Invarianzen und gelten als verlässlich.
   
   + Vor allem SIFT relativ langsam, SURF deutlich schneller
   
   + Beide benutzen reellwertige Deskriptoren -> aufwendige L2-Norm

* Es wurden daher einige Deskriptoren entwickelt, die Binärstrings sind (BRIEF, 
  ORB, KAZE-Deskriptor) 

   + schneller Vergleich (Hamming, bitwise-and), was auf schwächerer Hardware von Vorteil ist

* AKAZE: Detektor mit binären Deskriptoren, und schneller als SURF sein soll,
  mindestens so guter Genauigkeit

* Hier muss man auch etwas herumspielen, bis man die richtigen Parameter 
  gefunden hat und dabei Performanz gegen Featurezahl und -salienz abwägen

* aufgrund sehr unterschiedlicher Parametertypen schwer, Detektoren 
  zu vergleichen, AKAZE scheint zumindest etwas schneller zu sein

* Wir benutzen also AKAZE, um auf den Bildern Korrespondenzen zu schätzen, 
  eventuell kann man noch einen Ratio-test machen (tausche Laufzeit gegen Güte)

* Bisher hängen die Ergebnisse noch stark von der Art des Detektors und den 
  Parametern ab



Nun haben wir eigentlich alles: Korrespondenzen und Methode zur Versatzberechnung


 

ABLAUF



* Nach Beseitigung der Probleme sieht es dann in etwa so aus [GRAFIK ERLÄUTERN]



DEMO



* Nun noch kurz eine Demo der bisher vorhandenen Funktionalität 
* blub blub blub



STAND



* Es gibt schon

   + ein user interface (ist alles recht übersichtlich)

   + eine erste Hilfestellung durch das Kantenoverlay

   + eine experimentelle Implementierung der Versatzschätzung für 2 Bilder, ist 
   aber noch nicht integriert

   + Ansatz zum Upload auf einen mockup-server von Tilo Wiedera

   + Wie gesehen, einige theoretische Ansätze zu Fertigstellung

* Man kann noch vieles machen

   + Man sollte Ergebnisse anzeigen und Metadaten eingeben können

   + Die Assistenz ist obv noch nicht implementiert

   + Fotos durch post-processing besser zur Deckung bringen

   + Misc




